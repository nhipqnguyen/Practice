1. What are the key requirements for becoming a Data Analyst?
- Be well-versed with programming languages (XML, Javascript, or ETL frameworks), databases - (SQL, SQLite, Db2, etc.), and also have extensive knowledge on reporting packages (Business Objects).
- Be able to analyze, organize, collect and disseminate Big Data efficiently.
- You must have substantial technical knowledge in fields like database design, data mining, and segmentation techniques.
- Have a sound knowledge of statistical packages for analyzing massive datasets such as SAS, Excel, and SPSS, to name a few.

2. What are the important responsibilities of a data analyst?
- This is the most commonly asked data analyst interview question. You must have a clear idea as to what your job entails.
- A data analyst is required to perform the following tasks:
  - Collect and interpret data from multiple sources and analyze results.
  - Filter and “clean” data gathered from multiple sources.
  - Offer support to every aspect of data analysis.
  - Analyze complex datasets and identify the hidden patterns in them.
  - Keep databases secured.

3. What does “Data Cleansing” mean? What are the best ways to practice this?
- If you are sitting for a data analyst job, this is one of the most frequently asked data analyst interview questions.
- Data cleansing primarily refers to the process of detecting and removing errors and inconsistencies from the data to improve data quality.
- The best ways to clean data are:
  - Segregating data, according to their respective attributes.
  - Breaking large chunks of data into small datasets and then cleaning them.
  - Analyzing the statistics of each data column.
  - Creating a set of utility functions or scripts for dealing with common cleaning tasks.
  - Keeping track of all the data cleansing operations to facilitate easy addition or removal from the datasets, if required.

4. Name the best tools used for data analysis.
- Tableau
- Google Fusion Tables
- Google Search Operators
- KNIME
- RapidMiner
- Solver
- OpenRefine
- NodeXL
- io

5. What is the difference between data profiling and data mining?
- Data Profiling focuses on analyzing individual attributes of data, thereby providing valuable information on data attributes such as data type, frequency, length, along with their discrete values and value ranges. On the contrary, data mining aims to identify unusual records, analyze data clusters, and sequence discovery, to name a few.

6. What is KNN imputation method?
- KNN imputation method seeks to impute the values of the missing attributes using those attribute values that are nearest to the missing attribute values. The similarity between two attribute values is determined using the distance function.

7. What should a data analyst do with missing or suspected data?
- Use data analysis strategies like deletion method, single imputation methods, and model-based methods to detect missing data.
- Prepare a validation report containing all information about the suspected or missing data.
- Scrutinize the suspicious data to assess their validity.
- Replace all the invalid data (if any) with a proper validation code.

8. Name the different data validation methods used by data analysts.
There are many ways to validate datasets. Some of the most commonly used data validation methods by Data Analysts include: 
- Field Level Validation – In this method, data validation is done in each field as and when a user enters the data. It helps to correct the errors as you go.
- Form Level Validation – In this method, the data is validated after the user completes the form and submits it. It checks the entire data entry form at once, validates all the fields in it, and highlights the errors (if any) so that the user can correct it. 
- Data Saving Validation – This data validation technique is used during the process of saving an actual file or database record. Usually, it is done when multiple data entry forms must be validated. 
- Search Criteria Validation – This validation technique is used to offer the user accurate and related matches for their searched keywords or phrases. The main purpose of this validation method is to ensure that the user’s search queries can return the most relevant results.

9. Define Outlier
- A data analyst interview question and answers guide will not complete without this question. An outlier is a term commonly used by data analysts when referring to a value that appears to be far removed and divergent from a set pattern in a sample. There are two kinds of outliers – Univariate and Multivariate.
- The two methods used for detecting outliers are:
  - Box plot method – According to this method, if the value is higher or lesser than 1.5*IQR (interquartile range), such that it lies above the upper quartile (Q3) or below the lower quartile (Q1), the value is an outlier.
  - Standard deviation method – This method states that if a value is higher or lower than mean ± (3*standard deviation), it is an outlier.

10. What is “Clustering?” Name the properties of clustering algorithms.
Clustering is a method in which data is classified into clusters and groups. A clustering algorithm has the following properties:
- Hierarchical or flat
- Hard and soft
- Iterative
- Disjunctive

11. What is K-mean Algorithm?
- K-mean is a partitioning technique in which objects are categorized into K groups. In this algorithm, the clusters are spherical with the data points are aligned around that cluster, and the variance of the clusters is similar to one another.

12. Define “Collaborative Filtering”.
- Collaborative filtering is an algorithm that creates a recommendation system based on the behavioral data of a user. For instance, online shopping sites usually compile a list of items under “recommended for you” based on your browsing history and previous purchases. The crucial components of this algorithm include users, objects, and their interest.

13. Name the statistical methods that are highly beneficial for data analysts?
- Bayesian method
- Markov process
- Simplex algorithm
- Imputation
- Spatial and cluster processes
- Rank statistics, percentile, outliers detection
- Mathematical optimization

14. What is an N-gram?
- An n-gram is a connected sequence of n items in a given text or speech. Precisely, an N-gram is a probabilistic language model used to predict the next item in a particular sequence, as in (n-1).

15. What is a hash table collision? How can it be prevented?
- This is one of the important data analyst interview questions. When two separate keys hash to a common value, a hash table collision occurs. This means that two different data cannot be stored in the same slot.
- Hash collisions can be avoided by:
  - Separate chaining – In this method, a data structure is used to store multiple items hashing to a common slot.
  - Open addressing – This method seeks out empty slots and stores the item in the first empty slot available.